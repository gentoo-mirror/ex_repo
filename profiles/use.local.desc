# This file is deprecated as per GLEP 56 in favor of metadata.xml.
# Please add your descriptions to your package's metadata.xml ONLY.
# * generated automatically using pmaint *

dev-cpp/kokkos:cuda - Enable the CUDA execution space. Any Kokkos application compiled for CUDA embeds CUDA code via template metaprogramming. Thus, the whole application must be built with a CUDA-capable compiler. (At the moment, the only such compilers are NVIDIAâ€™s NVCC and Clang 10.0+)
dev-cpp/kokkos:hip - Enable the HIP execution space. Requires a compiler capable of understanding HIP.
dev-cpp/kokkos:openmp - Enable the OpenMP execution space. Requires the compiler to support OpenMP (e.g., -fopenmp).
dev-cpp/kokkos:serial - Enable the Serial execution space.(Single thread without parallelization support)
dev-cpp/kokkos:sycl - Enable the SYCL execution space. Requires a compiler capable of understanding SYCL.
dev-cpp/kokkos:threads - Enable the C++ Threads execution space. (aka multithread support)
dev-cpp/llama-cpp:accelerate - ggml: enable Accelerate framework
dev-cpp/llama-cpp:blas - ggml: use BLAS; for using specific vendor check https://wiki.gentoo.org/wiki/Blas-lapack-switch
dev-cpp/llama-cpp:cann - ggml: use CANN
dev-cpp/llama-cpp:cpu - ggml: enable CPU backend
dev-cpp/llama-cpp:cpu_native - ggml: enable CPU backend; it will add `-march=native` by itself. CPU_FLAGS_* consistency is expected to be user's problem
dev-cpp/llama-cpp:cuda - ggml: use CUDA ; use CMAKE_EXTRA_CACHE_FILE env variable (check https://gitweb.gentoo.org/repo/gentoo.git/tree/eclass/cmake.eclass ) to specify next variables: CMAKE_CUDA_ARCHITECTURES ( use next command to find native architecture: `nvidia-smi --query-gpu=compute_cap --format=csv | tail -n 1 | sed -e 's/\.//g'` ) ; GGML_CUDA_PEER_MAX_BATCH_SIZE ggml: max. batch size for using peer access, default: 128
dev-cpp/llama-cpp:cuda_f16 - ggml: use 16 bit floats for some calculations
dev-cpp/llama-cpp:cuda_fa_all_quants - ggml: compile all quants for FlashAttention
dev-cpp/llama-cpp:cuda_force_cublas - ggml: always use cuBLAS instead of mmq kernels
dev-cpp/llama-cpp:cuda_force_mmq - ggml: use mmq kernels instead of cuBLAS
dev-cpp/llama-cpp:cuda_graphs - ggml: use CUDA graphs (llama.cpp only)
dev-cpp/llama-cpp:cuda_no_peer_copy - ggml: do not use peer to peer copies
dev-cpp/llama-cpp:cuda_no_vmm - ggml: do not try to use CUDA VMM
dev-cpp/llama-cpp:examples - ggml: build examples; "llama: build examples"
dev-cpp/llama-cpp:hip - ggml: use HIP
dev-cpp/llama-cpp:hip_graphs - ggml: use HIP graph, experimental, slow
dev-cpp/llama-cpp:hip_no_vmm - ggml: do not try to use HIP VMM
dev-cpp/llama-cpp:hip_uma - ggml: use HIP unified memory architecture : from docs/build.md : On Linux it is also possible to use unified memory architecture (UMA) to share main memory between the CPU and integrated GPU by setting -DGGML_HIP_UMA=ON. However, this hurts performance for non-integrated GPUs (but enables working with integrated GPUs).
dev-cpp/llama-cpp:kleidiai - ggml: use kleidiai optimized kernels if applicable" GUYS ON ARM64 or ARM, enable this!!!!!!!!!
dev-cpp/llama-cpp:kompute - ggml: use Kompute
dev-cpp/llama-cpp:llamafile - ggml: use LLAMAFILE
dev-cpp/llama-cpp:metal - ggml: use Metal; use CMAKE_EXTRA_CACHE_FILE env variable (check https://gitweb.gentoo.org/repo/gentoo.git/tree/eclass/cmake.eclass ) to specify next variables: ggml: metal minimum macOS version GGML_METAL_MACOSX_VERSION_MIN ; ggml: metal standard version (-std flag) GGML_METAL_STD
dev-cpp/llama-cpp:metal_embed_library - ggml: embed Metal library
dev-cpp/llama-cpp:metal_ndebug - ggml: disable Metal debugging
dev-cpp/llama-cpp:metal_shader_debug - ggml: compile Metal with -fno-fast-math
dev-cpp/llama-cpp:metal_use_bf16 - ggml: use bfloat if available
dev-cpp/llama-cpp:musa - ggml: use MUSA
dev-cpp/llama-cpp:opencl - ggml: use OpenCL
dev-cpp/llama-cpp:opencl_embed_kernels - ggml: embed kernels
dev-cpp/llama-cpp:opencl_profiling - ggml: use OpenCL profiling (increases overhead)
dev-cpp/llama-cpp:opencl_use_adreno_kernels - ggml: use optimized kernels for Adreno
dev-cpp/llama-cpp:openmp - ggml: use OpenMP
dev-cpp/llama-cpp:rpc - ggml: use RPC
dev-cpp/llama-cpp:server - ggml: build examples ; "llama: build server example"
dev-cpp/llama-cpp:static - static build?
dev-cpp/llama-cpp:sycl - ggml: use SYCL ; use CMAKE_EXTRA_CACHE_FILE env variable (check https://gitweb.gentoo.org/repo/gentoo.git/tree/eclass/cmake.eclass ) to specify next variables: GGML_SYCL_TARGET "INTEL" "ggml: sycl target device" ; GGML_SYCL_DEVICE_ARCH "" "ggml: sycl device architecture"
dev-cpp/llama-cpp:sycl_f16 - ggml: use 16 bit floats for sycl calculations
dev-cpp/llama-cpp:sycl_target_amdgpu - sycl_target_amdgpu
dev-cpp/llama-cpp:sycl_target_intelgpu - sycl_target_intelgpu
dev-cpp/llama-cpp:sycl_target_nvidia - sycl_target_nvidia
dev-cpp/llama-cpp:sycl_via_oneapi - sycl_via_oneapi
dev-cpp/llama-cpp:sycl_via_onemkl - sycl_via_onemkl
dev-cpp/llama-cpp:test - ggml: build tests; "llama: build tests"
dev-cpp/llama-cpp:vulkan - ggml: use Vulkan ; use CMAKE_EXTRA_CACHE_FILE env variable (check https://gitweb.gentoo.org/repo/gentoo.git/tree/eclass/cmake.eclass ) to specify next variables: GGML_VULKAN_SHADERS_GEN_TOOLCHAIN "" "ggml: toolchain file for vulkan-shaders-gen"
dev-cpp/llama-cpp:vulkan_check_results - ggml: run Vulkan op checks
dev-cpp/llama-cpp:vulkan_debug - ggml: enable Vulkan debug output
dev-cpp/llama-cpp:vulkan_memory_debug - ggml: enable Vulkan memory debug output
dev-cpp/llama-cpp:vulkan_perf - ggml: enable Vulkan perf output
dev-cpp/llama-cpp:vulkan_run_tests - ggml: run Vulkan tests
dev-cpp/llama-cpp:vulkan_shader_debug_info - ggml: enable Vulkan shader debug info
dev-cpp/llama-cpp:vulkan_validate - ggml: enable Vulkan validation
dev-libs/boost:boost_context_fcontext - Boost.Context implementation. Default on not-windows OS. https://www.boost.org/doc/libs/1_87_0/libs/context/doc/html/context/cc/implementations__fcontext_t__ucontext_t_and_winfiber.html
dev-libs/boost:boost_context_ucontext - Boost.Context implementation. Deprecated. https://www.boost.org/doc/libs/1_87_0/libs/context/doc/html/context/cc/implementations__fcontext_t__ucontext_t_and_winfiber.html
dev-libs/boost:boost_context_winfib - Boost.Context implementation. Windows only. https://www.boost.org/doc/libs/1_87_0/libs/context/doc/html/context/cc/implementations__fcontext_t__ucontext_t_and_winfiber.html
dev-libs/boost:boost_iostream_bzip2 - When ON, enables BZip2 support. Defaults to ON when libbzip2 is found, OFF otherwise.
dev-libs/boost:boost_iostream_lzma - When ON, enables LZMA support. Defaults to ON when liblzma is found, OFF otherwise.
dev-libs/boost:boost_iostream_zlib - When ON, enables ZLib support. Defaults to ON when zlib is found, OFF otherwise.
dev-libs/boost:boost_iostream_zstd - When ON, enables Zstd support. Defaults to ON when libzstd is found, OFF otherwise.
dev-libs/boost:boost_locale_iconv - When ON, enables the Iconv backend. Defaults to ON when iconv is found, OFF otherwise. https://github.com/boostorg/cmake/tree/7f5336b3bf8067bb40da4e8b9940c133271e938a?tab=readme-ov-file
dev-libs/boost:boost_locale_icu - When ON, enables the ICU backend. Defaults to ON when ICU is found, OFF otherwise. https://github.com/boostorg/cmake/tree/7f5336b3bf8067bb40da4e8b9940c133271e938a?tab=readme-ov-file
dev-libs/boost:boost_locale_posix - When ON, enables the POSIX backend. Defaults to ON on POSIX systems, OFF otherwise. https://github.com/boostorg/cmake/tree/7f5336b3bf8067bb40da4e8b9940c133271e938a?tab=readme-ov-file
dev-libs/boost:boost_locale_std - When ON, enables the std::locale backend. Defaults to ON. https://github.com/boostorg/cmake/tree/7f5336b3bf8067bb40da4e8b9940c133271e938a?tab=readme-ov-file
dev-libs/boost:boost_locale_winapi - When ON, enables the Windows API backend. Defaults to ON under Windows, OFF otherwise. https://github.com/boostorg/cmake/tree/7f5336b3bf8067bb40da4e8b9940c133271e938a?tab=readme-ov-file
dev-libs/boost:boost_thread_posix - Threading API, pthread or win32. Defaults to win32 under Windows, pthread otherwise.
dev-libs/boost:boost_thread_win32 - Threading API, pthread or win32. Defaults to win32 under Windows, pthread otherwise.
dev-libs/boost:mpi - Set to ON if Boost libraries depending on MPI should be built. https://github.com/boostorg/cmake/tree/7f5336b3bf8067bb40da4e8b9940c133271e938a?tab=readme-ov-file
dev-libs/boost:python - Set to ON if Boost libraries depending on Python should be built. https://github.com/boostorg/cmake/tree/7f5336b3bf8067bb40da4e8b9940c133271e938a?tab=readme-ov-file
dev-libs/boost:static - Set to ON if Boost libraries should be build as static libraries. https://github.com/boostorg/cmake/tree/7f5336b3bf8067bb40da4e8b9940c133271e938a?tab=readme-ov-file
dev-libs/simdjson:all-impls - Enable all (possibly supported by CHOST) implementations rather than only of them.
dev-libs/simdjson:shared-libs - Make shared library. If you are on windows, enable only one of `static-libs` and `shared-libs`
dev-libs/simdjson:singleheader - Make simdjson library out of amalgamated one simdjson.hpp and one simdjson.cpp, instead of whole bunch of files in include/ and src/ . Expected to be more stable.
dev-libs/simdjson:static-libs - Make static library. If you are on windows, enable only one of `static-libs` and `shared-libs`
dev-libs/simdjson:tools - Build and install extra command line tools
